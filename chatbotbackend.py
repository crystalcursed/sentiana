# -*- coding: utf-8 -*-
"""ChatbotBackend 0.1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WgPBrZaVC4dbVsEqii3dlTwrgQEyUB7u
"""

import nltk
nltk.download('punkt')
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.stem import PorterStemmer
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')
#pip install flask
from flask import Flask, render_template
app=Flask(__name__)
@app.route('/')
def main():
  return render_template('front.html')
#pip install spacy
#pip install negspacy
#pip install spacy_stanza
#import spacy
#import stanza
#import spacy_stanza
#from negspacy.negation import Negex
#from negspacy.termsets import termset

#import time
#import pickle
#import tensorflow as tf
#gpus = tf.config.experimental.list_physical_devices('GPU')
#if gpus:
    # only use GPU memory that we need, not allocate all the GPU memory
    #tf.config.experimental.set_memory_growth(gpus[0], enable=True)

#import tqdm
#import numpy as np
#from tensorflow.keras.preprocessing.text import Tokenizer
#from tensorflow.keras.preprocessing.sequence import pad_sequences
#from tensorflow.keras.utils import to_categorical
#from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard
#from sklearn.model_selection import train_test_split
#from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense
#from tensorflow.keras.models import Sequential
#from tensorflow.keras.metrics import Recall, Precision

#pip install insta-scrape

#from instascrape import *

word_data = "I do not like dwayne johnson but john cena likes bollywood"
nltk_tokens = nltk.word_tokenize(word_data)
print (nltk_tokens)

#Here we tokenized the text to seperate the sentence into different parts

tokens_without_sw = [word for word in nltk_tokens if not word in stopwords.words()]

print(tokens_without_sw)

ps = PorterStemmer()
for w in nltk_tokens:
	rootWord=ps.stem(w)
	print(rootWord)

polar= SentimentIntensityAnalyzer()
filter=(' '.join(nltk_tokens))
print(filter)
score=polar.polarity_scores(filter)
print(score)
if score["neg"]==0:
  print("This is a positive Sentence")
else:
  print("This is a negative sentence")

#nlp_model = spacy_stanza.load_pipeline('en')

#nlp_model.add_pipe("negex", config={"ent_types":["PERSON","ORG","CARDINAL", "DATE", "EVENT", "LANGUAGE", "PRODUCT", "QUANTITY", "TIME", "WORK_OF_ART"]})

#sample = nlp_model(filter)

#for e in sample.ents:
  #print(e.text, e._.negex)

#if sample["e"]==True&&False
 #printf("The following sentence can be classified as sarcastic")

#SEQUENCE_LENGTH = 100 # the length of all sequences (number of words per sample)
#EMBEDDING_SIZE = 100  # Using 100-Dimensional GloVe embedding vectors
#TEST_SIZE = 0.25 # ratio of testing set

#BATCH_SIZE = 64
#EPOCHS = 10 # number of epochs

#label2int = {"ham": 0, "spam": 1}
#int2label = {0: "ham", 1: "spam"}

#google = Profile('https://www.instagram.com/google/')
#google_post = Post('https://www.instagram.com/p/CG0UU3ylXnv/')
#google_hashtag = Hashtag('https://www.instagram.com/explore/tags/google/')

# Scrape their respective data
#google.scrape()
#google_post.scrape()
#google_hashtag.scrape()

#print(google.followers)
#print(google_post['hashtags'])
#print(google_hashtag.amount_of_posts)

#def extract_features(word_list):
 #return dict([(word, True) for word in word_list])

#classifier = NaiveBayesClassifier.train(features_train)
#print("Accuracy of the classifier: ", nltk.classify.util.accuracy(classifier, features_test))

#Sample input reviews
#input_reviews = [
    #"Started off as the greatest series of all time, but had the worst ending of all time.",
    #"Exquisite. 'Big Little Lies' takes us to an incredible journey with its emotional and intriguing storyline.",
    #"I love Brooklyn 99 so much. It has the best crew ever!!",
    #"The Big Bang Theory and to me it's one of the best written sitcoms currently on network TV.",
    #"'Friends' is simply the best series ever aired. The acting is amazing.",
    #"SUITS is smart, sassy, clever, sophisticated, timely and immensely entertaining!",
    #"Cumberbatch is a fantastic choice for Sherlock Holmes-he is physically right (he fits the traditional reading of the character) and he is a damn good actor",
    #"What sounds like a typical agent hunting serial killer, surprises with great characters, surprising turning points and amazing cast."
    #"This is one of the most magical things I have ever had the fortune of viewing.",
    #"I don't recommend watching this at all!"
#]

#print("Predictions: ")

#for review in input_reviews:
    #print("\nReview:", review)
    #probdist = classifier.prob_classify(extract_features(review.split()))
    #pred_sentiment = probdist.max()

#print("Predictions: ")

#for review in input_reviews:
    #print("\nReview:", review)
    #probdist = classifier.prob_classify(extract_features(review.split()))
    #pred_sentiment = probdist.max()
    #print("Predicted sentiment: ", pred_sentiment)
    #print("Probability: ", round(probdist.prob(pred_sentiment), 2))